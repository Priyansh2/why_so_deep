{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores relation extraction by measuring common dependency paths between two entities that hold a given relation to each other -- here, the relation \"born_in\" between a PER entity and an GPE entity, using data from Wikipedia biographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import neuralcoref\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "# workaround if you are getting an error loading the sapcy 'en' module:\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(one, two):\n",
    "    \n",
    "    \"\"\" Get dependency path between two tokens in a sentence; return None if not reachable \"\"\"\n",
    "    \n",
    "    one_heads=[x for x in one.ancestors]\n",
    "    two_heads=[x for x in two.ancestors]\n",
    "    \n",
    "    up_path=[]\n",
    "    down_path=[]\n",
    "    up_path.append(one)\n",
    "    down_path.append(two)\n",
    "    \n",
    "    lca=None\n",
    "    for head in one_heads:\n",
    "        if head in two_heads:\n",
    "            lca=head\n",
    "            break\n",
    "            \n",
    "        up_path.append(head)\n",
    "\n",
    "    for head in two_heads:\n",
    "        if head == lca:\n",
    "            break\n",
    "    \n",
    "        down_path.append(head)\n",
    "   \n",
    "    if lca is None:\n",
    "        return None\n",
    "    \n",
    "    path=\"%s->%s<-%s\" % ('->'.join([\"%s\" % x.dep_ for x in up_path]), lca.text, '<-'.join([\"%s\" % x.dep_ for x in reversed(down_path)]))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_coref(entity1, clusters, target_entity):\n",
    "    \n",
    "    \"\"\" Given entities e1 and mention m2 of another entity, returns the mention for e1 closest to m2 \"\"\"\n",
    "    \n",
    "    targetCluster=None\n",
    "    for chain in clusters:\n",
    "        for mention in chain.mentions:\n",
    "            if mention.start <= entity1.start and mention.end >= entity1.end:\n",
    "                targetCluster=chain\n",
    "                break\n",
    "\n",
    "    if targetCluster is None:\n",
    "        return None\n",
    "\n",
    "    closestMention=None\n",
    "    dist=100\n",
    "    for mention in targetCluster:\n",
    "            sentDist=abs(target_entity.sent.start-mention.sent.start)\n",
    "            if sentDist < dist:\n",
    "                dist=sentDist\n",
    "                closestMention=mention\n",
    "            if sentDist == dist and closestMention is not None:\n",
    "                if abs(target_entity.start-mention.start) < abs(target_entity.start-closestMention.start):\n",
    "                    closetMention=mention\n",
    "    return closestMention\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. In class activity: here's [a Google spreadsheet](https://docs.google.com/spreadsheets/d/1PNDInP5JIqad9mOXwRUxGDZntvoUerX22QQcgFCJDxY/edit?usp=sharing) with the first 5 sentences from ~500 Wikipedia biographies.  Pick 10 rows of this spreadsheet and put your student ID in the \"Student ID\" column; then go through those 10 rows and read the document. If you can infer that a person from the People column was born in a place in the Places column, list that person in the \"PER BORN\" column and the place in the \"PLACE BORN\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training(filename):\n",
    "    \n",
    "    \"\"\" Read in training data for <person, place> tuples that express the \"born_in\" relation.\n",
    "    \n",
    "    -- Use coreference resolution to identity the person mention closest to the place mention.\n",
    "    -- Use dependency parsing to extract the syntactic path from that person mention to the place.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data=[]\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols=line.split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            doc=cols[1]\n",
    "            pers=cols[4]\n",
    "            place=cols[5].rstrip()\n",
    "            \n",
    "            if pers != \"\" and place != \"\":\n",
    "                doc=nlp(doc)\n",
    "\n",
    "                target_person=None\n",
    "                target_place=None\n",
    "                \n",
    "                # Annotations are at the type level, so let's anchor them to specific mentions\n",
    "                for entity in doc.ents:\n",
    "                    if entity.text == pers:\n",
    "                        target_person=entity\n",
    "                    elif entity.text == place:\n",
    "                        target_place=entity\n",
    "                \n",
    "                if target_person is not None and target_place is not None:\n",
    "                    \n",
    "                    # Use coreference to get person mention that's closest to the place (ideally in the same sentence).\n",
    "                    closest_person_mention=get_closest_coref(target_person, doc._.coref_clusters, target_place)\n",
    "                    if closest_person_mention is None:\n",
    "                        closest_person_mention=target_person\n",
    "                    \n",
    "                    path=get_path(closest_person_mention.root, target_place.root)\n",
    "                    \n",
    "                    # if a path can be found between the two\n",
    "                    if path is not None:\n",
    "                        data.append((pers, place, path, target_place.sent ))\n",
    "    return data     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this Google sheet as a tsv in `data/born.tsv` and execute the `read_training` function on it to read in the <person, place> tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=read_training(\"../data/born.tsv\")\n",
    "for data in trainingData:\n",
    "    print ('\\t'.join([str(x) for x in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Count the syntactic paths identified in the training data.  What are the two that are most frequently attested?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Write a function to read in a target file (containing one document per line) and a syntactic path and identify all people/places that are joined by that path. Hint: you can use the get_path functon defined above to retrieve the syntactic path between two tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations(filename, target_path):\n",
    "    \n",
    "    \"\"\" Extract new relations from a file.\n",
    "    Input: \n",
    "        - filename containing one document per line\n",
    "        - target_path: the syntactic dependency path connecting the person entity to the place entity\n",
    "    Output:\n",
    "        - a list of (person, place, path, sentence) tuples in the same format returned from `read_training`.\n",
    "    \n",
    "    \"\"\"\n",
    "    data=[]\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples=extract_relations(\"../data/wiki.bio.born.test.txt\", \"nsubjpass->born<-prep<-pobj\")\n",
    "for data in new_examples:\n",
    "    print ('\\t'.join([str(x) for x in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Execute `extract_relations` on `../data/wiki.bio.born.test.txt` and the two most frequent paths identified in the training data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
